[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  HYPOTHESIS TESTING",
    "section": "",
    "text": "1.1 Introduction to Hypothesis Testing\nHypothesis testing helps you make decisions based on data. In simple terms, it helps determine whether an assumption (hypothesis) about a population is likely to be true.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>HYPOTHESIS TESTING</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Hanck, C., M. Arnold, A. Gerber, and M. Schmelzer. 2021.\nIntroduction to Econometrics with r. Universität\nDuisburg-Essen. https://books.google.co.tz/books?id=YXmyzwEACAAJ.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "intro.html#estimation-of-the-population-mean",
    "href": "intro.html#estimation-of-the-population-mean",
    "title": "1  A REVIEW OF STATISTICS USING R",
    "section": "",
    "text": "Hanck, C., Arnold, M., Gerber, A., & Schmelzer, M. (2021). Introduction to Econometrics with R. Universität Duisburg-Essen. https://books.google.co.tz/books?id=YXmyzwEACAAJ",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A REVIEW OF STATISTICS USING R</span>"
    ]
  },
  {
    "objectID": "intro.html#types-of-hypothesis-tests",
    "href": "intro.html#types-of-hypothesis-tests",
    "title": "1  HYPOTHESIS TESTING",
    "section": "1.2 Types of Hypothesis Tests",
    "text": "1.2 Types of Hypothesis Tests\n\n1.2.1 t-test\nA t-test is used to compare means and is commonly used when the sample size is small, and the population variance is unknown.\nTypes of t-tests:\n\nOne-sample t-test: Tests whether the mean of a single group is equal to a known value.\nTwo-sample t-test (independent): Compares the means of two independent groups.\nPaired t-test: Compares means of the same group at different times/ two different points in time (before/after scenarios).\n\nKey Assumptions:\n\nThe data are approximately normally distributed.\nThe population variance is unknown.\nFor independent t-tests, the groups are independent of each other.\n\nWhen to use:\n\nSmall sample sizes (generally n &lt; 30).\nUnknown population variance.\nComparing two groups or two conditions.\n\n\n1.2.1.1 One-sample t-test\nLet’s say we have a group of students and want to test if their average test score is 70.\nSteps:\n\nNull hypothesis (Ho): The mean score = 70.\nAlternative hypothesis (Ha): The mean score != 70. (!= means is not equal)\n\n\n# Sample data (test scores)\nscores &lt;- c(68, 70, 75, 71, 69, 72, 66, 74, 73)\n\n# Perform one-sample t-test\nt.test(scores, mu = 70)\n#&gt; \n#&gt;  One Sample t-test\n#&gt; \n#&gt; data:  scores\n#&gt; t = 0.90874, df = 8, p-value = 0.39\n#&gt; alternative hypothesis: true mean is not equal to 70\n#&gt; 95 percent confidence interval:\n#&gt;  68.63326 73.14452\n#&gt; sample estimates:\n#&gt; mean of x \n#&gt;  70.88889\n\nmu = 70 is the value we are testing against. Look at the p-value in the output. If it’s less than 0.05, reject the null hypothesis.\n\n\n1.2.1.2 Two-sample t-test (Independent)\nYou want to compare the test scores between two groups: Group A and Group B.\nSteps:\n\nNull hypothesis (H₀): The mean score of Group A = Group B.\nAlternative hypothesis (H₁): The mean scores are different.\n\n\n# Sample data\ngroup_a &lt;- c(68, 70, 75, 71, 69)\ngroup_b &lt;- c(72, 74, 73, 76, 78)\n\n# Perform two-sample t-test\nt.test(group_a, group_b)\n#&gt; \n#&gt;  Welch Two Sample t-test\n#&gt; \n#&gt; data:  group_a and group_b\n#&gt; t = -2.4712, df = 7.8965, p-value = 0.03901\n#&gt; alternative hypothesis: true difference in means is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -7.7411294 -0.2588706\n#&gt; sample estimates:\n#&gt; mean of x mean of y \n#&gt;      70.6      74.6\n\n\n\n1.2.1.3 Paired t-test\nSuppose we want to check if a training program improves test scores. We have scores before and after the training for the same individuals.\nSteps:\n\nNull hypothesis (H₀): No change in mean scores.\nAlternative hypothesis (H₁): The mean scores are different before and after training.\n\n\n# Scores before and after training\nbefore &lt;- c(68, 70, 75, 71, 69)\nafter &lt;- c(70, 73, 78, 75, 72)\n\n# Perform paired t-test\nt.test(before, after, paired = TRUE)\n#&gt; \n#&gt;  Paired t-test\n#&gt; \n#&gt; data:  before and after\n#&gt; t = -9.4868, df = 4, p-value = 0.0006889\n#&gt; alternative hypothesis: true mean difference is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -3.877989 -2.122011\n#&gt; sample estimates:\n#&gt; mean difference \n#&gt;              -3\n\n\n\n\n1.2.2 Chi-square Test (for Categorical Data)\nThis test is used to assess relationships between categorical variables. For example, let’s say we want to test if there’s a relationship between gender and preference for a product.\nSteps:\n\nNull hypothesis (H₀): Gender and product preference are independent.\nAlternative hypothesis (H₁): Gender and product preference are related.\n\n\n# Create a contingency table\ngender &lt;- c('Male', 'Female', 'Male', 'Female', 'Male', 'Female')\npreference &lt;- c('Product A', 'Product B', 'Product B', 'Product A', 'Product B', 'Product A')\ntable_data &lt;- table(gender, preference)\n\n# Perform chi-square test\nchisq.test(table_data)\n#&gt; Warning in chisq.test(table_data): Chi-squared approximation may be\n#&gt; incorrect\n#&gt; \n#&gt;  Pearson's Chi-squared test with Yates' continuity correction\n#&gt; \n#&gt; data:  table_data\n#&gt; X-squared = 0, df = 1, p-value = 1\n\nUnderstanding the Output:\n\nt-value / Chi-squared value: This tells us how far the sample statistic is from the hypothesized population value.\np-value: The key value to focus on. If the p-value is less than your chosen significance level (commonly 0.05), reject the null hypothesis.\nConfidence Interval: This provides a range within which the true population parameter is likely to fall.\n\nCommon Pitfalls\n\nConfusing correlation with causation. Even if you find a significant relationship, it doesn’t imply one variable causes the other.\nOver-reliance on p-values. A small p-value doesn’t always mean practical significance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>HYPOTHESIS TESTING</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Using R for Introductory Econometrics",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html#introduction-to-hypothesis-testing",
    "href": "intro.html#introduction-to-hypothesis-testing",
    "title": "1  HYPOTHESIS TESTING",
    "section": "",
    "text": "Null Hypothesis (Ho): A default assumption (e.g., “There is no difference” or “There is no effect”).\nAlternative Hypothesis (Ha): The opposite of Ho (e.g., “There is a difference” or “There is an effect”).\np-value: The probability of observing the data if the null hypothesis is true. If this value is low (usually below 0.05), we reject the null hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>HYPOTHESIS TESTING</span>"
    ]
  },
  {
    "objectID": "intro.html#z-test",
    "href": "intro.html#z-test",
    "title": "1  HYPOTHESIS TESTING",
    "section": "1.3 z-test",
    "text": "1.3 z-test\nA z-test is similar to the t-test but is used when the sample size is large, and the population variance is known or can be approximated using the sample.\nKey Assumptions:\n\nThe sample size is large (n &gt; 30).\nThe population variance is known or approximated.\nData is normally distributed (or approximated to normal using the Central Limit Theorem for large samples).\n\nWhen to use:\n\nLarge sample sizes.\nKnown population variance.\nComparing sample means to population means.\n\nExample:\nTesting if the average test score of a class of 100 students is different from a known population mean of 70 when the population variance is known.\nThere is no built-in z-test function in base R, but you can calculate it manually as follows:\n\n# Z-test for a sample mean\nz &lt;- (mean - population_mean) / (std_dev / sqrt(n))\np_value &lt;- 2 * pnorm(-abs(z))  # two-tailed test",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>HYPOTHESIS TESTING</span>"
    ]
  },
  {
    "objectID": "intro.html#anova-analysis-of-variance",
    "href": "intro.html#anova-analysis-of-variance",
    "title": "1  HYPOTHESIS TESTING",
    "section": "1.5 ANOVA (Analysis of Variance)",
    "text": "1.5 ANOVA (Analysis of Variance)\nANOVA is used to compare the means of three or more groups to determine if at least one group differs significantly.\nTypes of ANOVA:\n\nOne-way ANOVA: Compares means across three or more independent groups based on one factor.\nTwo-way ANOVA: Compares means across groups based on two factors (e.g., time and treatment).\n\nKey Assumptions:\n\nThe groups are independent.\nThe data are normally distributed.\nHomogeneity of variances (the variance within each group is roughly equal).\n\nWhen to use:\n\nComparing means of three or more groups.\nDetermining whether any of the groups significantly differ from each other.\n\nExample: Testing whether the average exam scores of students from three different schools are the same or different.\n\n# Sample data\ngroup_1 &lt;- c(68, 70, 75)\ngroup_2 &lt;- c(71, 69, 72)\ngroup_3 &lt;- c(66, 74, 73)\n\n# Combine data into a data frame\ndata &lt;- data.frame(score = c(group_1, group_2, group_3),\n                   group = factor(rep(c('Group 1', 'Group 2', 'Group 3'), each = 3)))\n\n# Perform one-way ANOVA\nanova_result &lt;- aov(score ~ group, data = data)\nsummary(anova_result)\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; group        2   0.22   0.111    0.01   0.99\n#&gt; Residuals    6  68.67  11.444\n\nKey Differences:\n\n\n\n\n\n\n\n\n\n\nTest\nNumber of Groups\nPopulation Variance\nSample Size\nUse Case\n\n\n\n\nt-test\n1 or 2\nUnknown\nSmall (n &lt; 30)\nCompare the means of 1 or 2 groups.\n\n\nz-test\n1 or 2\nKnown or large n\nLarge (n &gt; 30)\nCompare means with known variance.\n\n\nANOVA\n3 or more\nUnknown\nAny\nCompare means across 3+ groups.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>HYPOTHESIS TESTING</span>"
    ]
  },
  {
    "objectID": "intro.html#f-test",
    "href": "intro.html#f-test",
    "title": "1  HYPOTHESIS TESTING",
    "section": "1.4 F-test",
    "text": "1.4 F-test\nAn F-test is used to compare the variances of two populations, or to assess whether a group of variables in a model significantly contributes to the outcome.\nCommon uses of the F-test:\n\nVariance Comparison: Compare the variances of two populations.\nANOVA (Analysis of Variance): The F-test is used to compare the variances between groups and within groups to determine if the means are significantly different.\nRegression Analysis: Used to determine whether the overall regression model fits the data well, and whether the independent variables together predict the outcome.\n\nKey Assumptions:\n\nThe data follows a normal distribution.\nThe samples are independent.\nThe populations have equal variances (for ANOVA).\n\nWhen to use:\n\nTo test if the variances of two populations are equal (variance comparison).\nTo determine if the means of three or more groups are significantly different (as part of ANOVA).\nTo assess the overall fit of a regression model.\n\nExample: Comparing the variances in test scores between two different teaching methods. Testing whether a regression model explains a significant portion of the variance in the outcome variable.\nR Example (Variance Comparison):\n\n# Sample data\ngroup1 &lt;- c(5.1, 5.8, 6.2, 5.9, 5.4)\ngroup2 &lt;- c(7.3, 7.6, 7.1, 7.8, 7.4)\n\n# Perform F-test to compare variances\nvar.test(group1, group2)\n#&gt; \n#&gt;  F test to compare two variances\n#&gt; \n#&gt; data:  group1 and group2\n#&gt; F = 2.5616, num df = 4, denom df = 4, p-value = 0.3845\n#&gt; alternative hypothesis: true ratio of variances is not equal to 1\n#&gt; 95 percent confidence interval:\n#&gt;   0.266712 24.603385\n#&gt; sample estimates:\n#&gt; ratio of variances \n#&gt;           2.561644\n\nR Example (ANOVA F-test):\nIn ANOVA, the F-test is used to determine if the between-group variance is significantly greater than the within-group variance.\n\n# Perform one-way ANOVA (F-test)\nanova_result &lt;- aov(score ~ group, data = data)\nsummary(anova_result)  # The output includes the F-statistic\n\nHow the F-test differs from t-test, z-test, and ANOVA:\n\nComparison to t-test:\n\nt-test compares means between two groups, while the F-test often compares variances between groups. The t-test can handle one or two groups, but the F-test is used in models with more variables or for variance comparison. In regression, the t-test is used for testing individual coefficients, while the F-test checks the overall model significance.\n\nComparison to z-test:\n\nBoth t-test and z-test focus on comparing means, but the z-test requires known variance, while the F-test compares variances or assesses model significance. z-tests are not typically used for variance comparison, which is where the F-test is useful.\n\nComparison to ANOVA:\n\nThe F-test is a core component of ANOVA, where it compares the variance between groups to the variance within groups. While ANOVA uses the F-test to test if the means of three or more groups are different, the F-test itself is a general test for comparing variances and model significance. Comparison to Regression Analysis:\nIn multiple regression analysis, an F-test is used to evaluate whether the model, as a whole, explains the dependent variable. It tells us if at least one predictor variable is statistically significant.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>HYPOTHESIS TESTING</span>"
    ]
  },
  {
    "objectID": "hypotesting.html",
    "href": "hypotesting.html",
    "title": "1  HYPOTHESIS TESTING",
    "section": "",
    "text": "1.1 Introduction to Hypothesis Testing\nHypothesis testing helps you make decisions based on data. In simple terms, it helps determine whether an assumption (hypothesis) about a population is likely to be true.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>HYPOTHESIS TESTING</span>"
    ]
  },
  {
    "objectID": "hypotesting.html#introduction-to-hypothesis-testing",
    "href": "hypotesting.html#introduction-to-hypothesis-testing",
    "title": "1  HYPOTHESIS TESTING",
    "section": "",
    "text": "Null Hypothesis (Ho): A default assumption (e.g., “There is no difference” or “There is no effect”).\nAlternative Hypothesis (Ha): The opposite of Ho (e.g., “There is a difference” or “There is an effect”).\np-value: The probability of observing the data if the null hypothesis is true. If this value is low (usually below 0.05), we reject the null hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>HYPOTHESIS TESTING</span>"
    ]
  },
  {
    "objectID": "hypotesting.html#types-of-hypothesis-tests",
    "href": "hypotesting.html#types-of-hypothesis-tests",
    "title": "1  HYPOTHESIS TESTING",
    "section": "1.2 Types of Hypothesis Tests",
    "text": "1.2 Types of Hypothesis Tests\n\n1.2.1 t-test\nA t-test is used to compare means and is commonly used when the sample size is small, and the population variance is unknown.\nTypes of t-tests:\n\nOne-sample t-test: Tests whether the mean of a single group is equal to a known value.\nTwo-sample t-test (independent): Compares the means of two independent groups.\nPaired t-test: Compares means of the same group at different times/ two different points in time (before/after scenarios).\n\nKey Assumptions:\n\nThe data are approximately normally distributed.\nThe population variance is unknown.\nFor independent t-tests, the groups are independent of each other.\n\nWhen to use:\n\nSmall sample sizes (generally n &lt; 30).\nUnknown population variance.\nComparing two groups or two conditions.\n\n\n1.2.1.1 One-sample t-test\nLet’s say we have a group of students and want to test if their average test score is 70.\nSteps:\n\nNull hypothesis (Ho): The mean score = 70.\nAlternative hypothesis (Ha): The mean score != 70. (!= means is not equal)\n\n\n# Sample data (test scores)\nscores &lt;- c(68, 70, 75, 71, 69, 72, 66, 74, 73)\n\n# Perform one-sample t-test\nt.test(scores, mu = 70)\n#&gt; \n#&gt;  One Sample t-test\n#&gt; \n#&gt; data:  scores\n#&gt; t = 0.90874, df = 8, p-value = 0.39\n#&gt; alternative hypothesis: true mean is not equal to 70\n#&gt; 95 percent confidence interval:\n#&gt;  68.63326 73.14452\n#&gt; sample estimates:\n#&gt; mean of x \n#&gt;  70.88889\n\nmu = 70 is the value we are testing against. Look at the p-value in the output. If it’s less than 0.05, reject the null hypothesis.\n\n\n1.2.1.2 Two-sample t-test (Independent)\nYou want to compare the test scores between two groups: Group A and Group B.\nSteps:\n\nNull hypothesis (H₀): The mean score of Group A = Group B.\nAlternative hypothesis (H₁): The mean scores are different.\n\n\n# Sample data\ngroup_a &lt;- c(68, 70, 75, 71, 69)\ngroup_b &lt;- c(72, 74, 73, 76, 78)\n\n# Perform two-sample t-test\nt.test(group_a, group_b)\n#&gt; \n#&gt;  Welch Two Sample t-test\n#&gt; \n#&gt; data:  group_a and group_b\n#&gt; t = -2.4712, df = 7.8965, p-value = 0.03901\n#&gt; alternative hypothesis: true difference in means is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -7.7411294 -0.2588706\n#&gt; sample estimates:\n#&gt; mean of x mean of y \n#&gt;      70.6      74.6\n\n\n\n1.2.1.3 Paired t-test\nSuppose we want to check if a training program improves test scores. We have scores before and after the training for the same individuals.\nSteps:\n\nNull hypothesis (H₀): No change in mean scores.\nAlternative hypothesis (H₁): The mean scores are different before and after training.\n\n\n# Scores before and after training\nbefore &lt;- c(68, 70, 75, 71, 69)\nafter &lt;- c(70, 73, 78, 75, 72)\n\n# Perform paired t-test\nt.test(before, after, paired = TRUE)\n#&gt; \n#&gt;  Paired t-test\n#&gt; \n#&gt; data:  before and after\n#&gt; t = -9.4868, df = 4, p-value = 0.0006889\n#&gt; alternative hypothesis: true mean difference is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -3.877989 -2.122011\n#&gt; sample estimates:\n#&gt; mean difference \n#&gt;              -3\n\n\n\n\n1.2.2 Chi-square Test (for Categorical Data)\nThis test is used to assess relationships between categorical variables. For example, let’s say we want to test if there’s a relationship between gender and preference for a product.\nSteps:\n\nNull hypothesis (H₀): Gender and product preference are independent.\nAlternative hypothesis (H₁): Gender and product preference are related.\n\n\n# Create a contingency table\ngender &lt;- c('Male', 'Female', 'Male', 'Female', 'Male', 'Female')\npreference &lt;- c('Product A', 'Product B', 'Product B', 'Product A', 'Product B', 'Product A')\ntable_data &lt;- table(gender, preference)\n\n# Perform chi-square test\nchisq.test(table_data)\n#&gt; Warning in chisq.test(table_data): Chi-squared approximation may be\n#&gt; incorrect\n#&gt; \n#&gt;  Pearson's Chi-squared test with Yates' continuity correction\n#&gt; \n#&gt; data:  table_data\n#&gt; X-squared = 0, df = 1, p-value = 1\n\nUnderstanding the Output:\n\nt-value / Chi-squared value: This tells us how far the sample statistic is from the hypothesized population value.\np-value: The key value to focus on. If the p-value is less than your chosen significance level (commonly 0.05), reject the null hypothesis.\nConfidence Interval: This provides a range within which the true population parameter is likely to fall.\n\nCommon Pitfalls\n\nConfusing correlation with causation. Even if you find a significant relationship, it doesn’t imply one variable causes the other.\nOver-reliance on p-values. A small p-value doesn’t always mean practical significance.\n\n\n\n1.2.3 z-test\nA z-test is similar to the t-test but is used when the sample size is large, and the population variance is known or can be approximated using the sample.\nKey Assumptions:\n\nThe sample size is large (n &gt; 30).\nThe population variance is known or approximated.\nData is normally distributed (or approximated to normal using the Central Limit Theorem for large samples).\n\nWhen to use:\n\nLarge sample sizes.\nKnown population variance.\nComparing sample means to population means.\n\nExample:\nTesting if the average test score of a class of 100 students is different from a known population mean of 70 when the population variance is known.\nThere is no built-in z-test function in base R, but you can calculate it manually as follows:\n\n# Z-test for a sample mean\nz &lt;- (mean - population_mean) / (std_dev / sqrt(n))\np_value &lt;- 2 * pnorm(-abs(z))  # two-tailed test\n\n\n\n1.2.4 F-test\nAn F-test is used to compare the variances of two populations, or to assess whether a group of variables in a model significantly contributes to the outcome.\nCommon uses of the F-test:\n\nVariance Comparison: Compare the variances of two populations.\nANOVA (Analysis of Variance): The F-test is used to compare the variances between groups and within groups to determine if the means are significantly different.\nRegression Analysis: Used to determine whether the overall regression model fits the data well, and whether the independent variables together predict the outcome.\n\nKey Assumptions:\n\nThe data follows a normal distribution.\nThe samples are independent.\nThe populations have equal variances (for ANOVA).\n\nWhen to use:\n\nTo test if the variances of two populations are equal (variance comparison).\nTo determine if the means of three or more groups are significantly different (as part of ANOVA).\nTo assess the overall fit of a regression model.\n\nExample: Comparing the variances in test scores between two different teaching methods. Testing whether a regression model explains a significant portion of the variance in the outcome variable.\nR Example (Variance Comparison):\n\n# Sample data\ngroup1 &lt;- c(5.1, 5.8, 6.2, 5.9, 5.4)\ngroup2 &lt;- c(7.3, 7.6, 7.1, 7.8, 7.4)\n\n# Perform F-test to compare variances\nvar.test(group1, group2)\n#&gt; \n#&gt;  F test to compare two variances\n#&gt; \n#&gt; data:  group1 and group2\n#&gt; F = 2.5616, num df = 4, denom df = 4, p-value = 0.3845\n#&gt; alternative hypothesis: true ratio of variances is not equal to 1\n#&gt; 95 percent confidence interval:\n#&gt;   0.266712 24.603385\n#&gt; sample estimates:\n#&gt; ratio of variances \n#&gt;           2.561644\n\nR Example (ANOVA F-test):\nIn ANOVA, the F-test is used to determine if the between-group variance is significantly greater than the within-group variance.\n\n# Perform one-way ANOVA (F-test)\nanova_result &lt;- aov(score ~ group, data = data)\nsummary(anova_result)  # The output includes the F-statistic\n\nHow the F-test differs from t-test, z-test, and ANOVA:\n\nComparison to t-test:\n\nt-test compares means between two groups, while the F-test often compares variances between groups. The t-test can handle one or two groups, but the F-test is used in models with more variables or for variance comparison. In regression, the t-test is used for testing individual coefficients, while the F-test checks the overall model significance.\n\nComparison to z-test:\n\nBoth t-test and z-test focus on comparing means, but the z-test requires known variance, while the F-test compares variances or assesses model significance. z-tests are not typically used for variance comparison, which is where the F-test is useful.\n\nComparison to ANOVA:\n\nThe F-test is a core component of ANOVA, where it compares the variance between groups to the variance within groups. While ANOVA uses the F-test to test if the means of three or more groups are different, the F-test itself is a general test for comparing variances and model significance. Comparison to Regression Analysis:\nIn multiple regression analysis, an F-test is used to evaluate whether the model, as a whole, explains the dependent variable. It tells us if at least one predictor variable is statistically significant.\n\n\n1.2.5 ANOVA (Analysis of Variance)\nANOVA is used to compare the means of three or more groups to determine if at least one group differs significantly.\nTypes of ANOVA:\n\nOne-way ANOVA: Compares means across three or more independent groups based on one factor.\nTwo-way ANOVA: Compares means across groups based on two factors (e.g., time and treatment).\n\nKey Assumptions:\n\nThe groups are independent.\nThe data are normally distributed.\nHomogeneity of variances (the variance within each group is roughly equal).\n\nWhen to use:\n\nComparing means of three or more groups.\nDetermining whether any of the groups significantly differ from each other.\n\nExample: Testing whether the average exam scores of students from three different schools are the same or different.\n\n# Sample data\ngroup_1 &lt;- c(68, 70, 75)\ngroup_2 &lt;- c(71, 69, 72)\ngroup_3 &lt;- c(66, 74, 73)\n\n# Combine data into a data frame\ndata &lt;- data.frame(score = c(group_1, group_2, group_3),\n                   group = factor(rep(c('Group 1', 'Group 2', 'Group 3'), each = 3)))\n\n# Perform one-way ANOVA\nanova_result &lt;- aov(score ~ group, data = data)\nsummary(anova_result)\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; group        2   0.22   0.111    0.01   0.99\n#&gt; Residuals    6  68.67  11.444\n\n\nKey Differences:\n\n\n\n\n\n\n\n\n\nTest\nNumber of Groups\nPopulation Variance\nSample Size\nUse Case\n\n\n\n\nt-test\n1 or 2\nUnknown\nSmall (n &lt; 30)\nCompare the means of 1 or 2 groups.\n\n\nz-test\n1 or 2\nKnown or large n\nLarge (n &gt; 30)\nCompare means with known variance.\n\n\nANOVA\n3 or more\nUnknown\nAny\nCompare means across 3+ groups.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>HYPOTHESIS TESTING</span>"
    ]
  },
  {
    "objectID": "hypotesting.html#z-test",
    "href": "hypotesting.html#z-test",
    "title": "1  HYPOTHESIS TESTING",
    "section": "1.3 z-test",
    "text": "1.3 z-test\nA z-test is similar to the t-test but is used when the sample size is large, and the population variance is known or can be approximated using the sample.\nKey Assumptions:\n\nThe sample size is large (n &gt; 30).\nThe population variance is known or approximated.\nData is normally distributed (or approximated to normal using the Central Limit Theorem for large samples).\n\nWhen to use:\n\nLarge sample sizes.\nKnown population variance.\nComparing sample means to population means.\n\nExample:\nTesting if the average test score of a class of 100 students is different from a known population mean of 70 when the population variance is known.\nThere is no built-in z-test function in base R, but you can calculate it manually as follows:\n\n# Z-test for a sample mean\nz &lt;- (mean - population_mean) / (std_dev / sqrt(n))\np_value &lt;- 2 * pnorm(-abs(z))  # two-tailed test",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>HYPOTHESIS TESTING</span>"
    ]
  },
  {
    "objectID": "hypotesting.html#f-test",
    "href": "hypotesting.html#f-test",
    "title": "1  HYPOTHESIS TESTING",
    "section": "1.4 F-test",
    "text": "1.4 F-test\nAn F-test is used to compare the variances of two populations, or to assess whether a group of variables in a model significantly contributes to the outcome.\nCommon uses of the F-test:\n\nVariance Comparison: Compare the variances of two populations.\nANOVA (Analysis of Variance): The F-test is used to compare the variances between groups and within groups to determine if the means are significantly different.\nRegression Analysis: Used to determine whether the overall regression model fits the data well, and whether the independent variables together predict the outcome.\n\nKey Assumptions:\n\nThe data follows a normal distribution.\nThe samples are independent.\nThe populations have equal variances (for ANOVA).\n\nWhen to use:\n\nTo test if the variances of two populations are equal (variance comparison).\nTo determine if the means of three or more groups are significantly different (as part of ANOVA).\nTo assess the overall fit of a regression model.\n\nExample: Comparing the variances in test scores between two different teaching methods. Testing whether a regression model explains a significant portion of the variance in the outcome variable.\nR Example (Variance Comparison):\n\n# Sample data\ngroup1 &lt;- c(5.1, 5.8, 6.2, 5.9, 5.4)\ngroup2 &lt;- c(7.3, 7.6, 7.1, 7.8, 7.4)\n\n# Perform F-test to compare variances\nvar.test(group1, group2)\n#&gt; \n#&gt;  F test to compare two variances\n#&gt; \n#&gt; data:  group1 and group2\n#&gt; F = 2.5616, num df = 4, denom df = 4, p-value = 0.3845\n#&gt; alternative hypothesis: true ratio of variances is not equal to 1\n#&gt; 95 percent confidence interval:\n#&gt;   0.266712 24.603385\n#&gt; sample estimates:\n#&gt; ratio of variances \n#&gt;           2.561644\n\nR Example (ANOVA F-test):\nIn ANOVA, the F-test is used to determine if the between-group variance is significantly greater than the within-group variance.\n\n# Perform one-way ANOVA (F-test)\nanova_result &lt;- aov(score ~ group, data = data)\nsummary(anova_result)  # The output includes the F-statistic\n\nHow the F-test differs from t-test, z-test, and ANOVA:\n\nComparison to t-test:\n\nt-test compares means between two groups, while the F-test often compares variances between groups. The t-test can handle one or two groups, but the F-test is used in models with more variables or for variance comparison. In regression, the t-test is used for testing individual coefficients, while the F-test checks the overall model significance.\n\nComparison to z-test:\n\nBoth t-test and z-test focus on comparing means, but the z-test requires known variance, while the F-test compares variances or assesses model significance. z-tests are not typically used for variance comparison, which is where the F-test is useful.\n\nComparison to ANOVA:\n\nThe F-test is a core component of ANOVA, where it compares the variance between groups to the variance within groups. While ANOVA uses the F-test to test if the means of three or more groups are different, the F-test itself is a general test for comparing variances and model significance. Comparison to Regression Analysis:\nIn multiple regression analysis, an F-test is used to evaluate whether the model, as a whole, explains the dependent variable. It tells us if at least one predictor variable is statistically significant.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>HYPOTHESIS TESTING</span>"
    ]
  },
  {
    "objectID": "hypotesting.html#anova-analysis-of-variance",
    "href": "hypotesting.html#anova-analysis-of-variance",
    "title": "1  HYPOTHESIS TESTING",
    "section": "1.5 ANOVA (Analysis of Variance)",
    "text": "1.5 ANOVA (Analysis of Variance)\nANOVA is used to compare the means of three or more groups to determine if at least one group differs significantly.\nTypes of ANOVA:\n\nOne-way ANOVA: Compares means across three or more independent groups based on one factor.\nTwo-way ANOVA: Compares means across groups based on two factors (e.g., time and treatment).\n\nKey Assumptions:\n\nThe groups are independent.\nThe data are normally distributed.\nHomogeneity of variances (the variance within each group is roughly equal).\n\nWhen to use:\n\nComparing means of three or more groups.\nDetermining whether any of the groups significantly differ from each other.\n\nExample: Testing whether the average exam scores of students from three different schools are the same or different.\n\n# Sample data\ngroup_1 &lt;- c(68, 70, 75)\ngroup_2 &lt;- c(71, 69, 72)\ngroup_3 &lt;- c(66, 74, 73)\n\n# Combine data into a data frame\ndata &lt;- data.frame(score = c(group_1, group_2, group_3),\n                   group = factor(rep(c('Group 1', 'Group 2', 'Group 3'), each = 3)))\n\n# Perform one-way ANOVA\nanova_result &lt;- aov(score ~ group, data = data)\nsummary(anova_result)\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; group        2   0.22   0.111    0.01   0.99\n#&gt; Residuals    6  68.67  11.444\n\n\nKey Differences:\n\n\n\n\n\n\n\n\n\nTest\nNumber of Groups\nPopulation Variance\nSample Size\nUse Case\n\n\n\n\nt-test\n1 or 2\nUnknown\nSmall (n &lt; 30)\nCompare the means of 1 or 2 groups.\n\n\nz-test\n1 or 2\nKnown or large n\nLarge (n &gt; 30)\nCompare means with known variance.\n\n\nANOVA\n3 or more\nUnknown\nAny\nCompare means across 3+ groups.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>HYPOTHESIS TESTING</span>"
    ]
  }
]